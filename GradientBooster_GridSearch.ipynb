{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Booster Classifier with Grid Search\n",
    "\n",
    "This assignment is based on a data challenge from the Michigan Data Science Team ([MDST](http://midas.umich.edu/mdst/)). All data for this assignment has been provided through the [Detroit Open Data Portal](https://data.detroitmi.gov/), and was collated into two data files for use in training and validating the model: train.csv and test.csv\n",
    "\n",
    "This demostrates using `GradientBoostingClassifier` and `GridSearchCV` from `sklearn`. The grid search is used to find some appropriate hyperparameters to use in the final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def gradient_predictor():\n",
    "\n",
    "    # Custom code\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import metrics   #Additional scklearn functions\n",
    "    from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "\n",
    "    X_train = pd.read_csv('train.csv',encoding = 'ISO-8859-1', dtype={'zip_code' : object, 'non_us_str_code' : object, 'grafitti_status' : object})\n",
    "    X_test = pd.read_csv('test.csv',encoding = 'ISO-8859-1')\n",
    "    X_train = X_train[pd.notnull(X_train.compliance)]   \n",
    "\n",
    "    # First we'll do the feature preparation\n",
    "    \n",
    "    # Drop Train only fields\n",
    "    X_train = X_train.drop(['payment_amount', 'payment_date', 'payment_status', 'balance_due',\n",
    "                                  'collection_status', 'compliance_detail'], axis=1)\n",
    "\n",
    "    # Drop fields intutively too variable to carry information\n",
    "    X_train = X_train.drop(['ticket_id', 'inspector_name', 'violator_name', 'violation_street_number', \n",
    "                            'violation_street_name', 'violation_zip_code', 'mailing_address_str_number', \n",
    "                            'mailing_address_str_number', 'mailing_address_str_name', 'city', 'state', \n",
    "                            'zip_code', 'non_us_str_code', 'country'], axis=1)\n",
    "    \n",
    "    test_ticket_id = X_test['ticket_id']\n",
    "    X_test = X_test.drop(['ticket_id', 'inspector_name', 'violator_name', 'violation_street_number', \n",
    "                          'violation_street_name', 'violation_zip_code', 'mailing_address_str_number', \n",
    "                          'mailing_address_str_number', 'mailing_address_str_name', 'city', 'state', \n",
    "                          'zip_code', 'non_us_str_code', 'country'], axis=1)\n",
    "\n",
    "    # Drop fields with too much variation or null\n",
    "    X_train = X_train.drop(['violation_code', 'violation_description', 'grafitti_status'], axis=1)\n",
    "    X_test = X_test.drop(['violation_code', 'violation_description', 'grafitti_status'], axis=1)\n",
    "\n",
    "    # Drop fields that intuitively colinear with judgment_amount\n",
    "    X_train = X_train.drop(['fine_amount', 'admin_fee', 'state_fee', 'late_fee', 'discount_amount',\n",
    "                            'clean_up_cost'], axis=1) \n",
    "    X_test = X_test.drop(['fine_amount', 'admin_fee', 'state_fee', 'late_fee', 'discount_amount',\n",
    "                          'clean_up_cost'], axis=1) \n",
    "\n",
    "    # Time variables convert to duration continuous variable\n",
    "    tmp1 = pd.to_datetime(X_train['ticket_issued_date'], format='%Y-%m-%d').dt.date \n",
    "    tmp2 = pd.to_datetime(X_train['hearing_date'], format='%Y-%m-%d').dt.date\n",
    "    tmp3 = tmp2 - tmp1\n",
    "    X_train['duration'] = tmp3.dt.days \n",
    "    X_train['duration'] = X_train['duration'].where(X_train['duration']>0, other=0)\n",
    "    X_train = X_train.drop(['ticket_issued_date', 'hearing_date'], axis=1) \n",
    "\n",
    "    tmp1 = pd.to_datetime(X_test['ticket_issued_date'], format='%Y-%m-%d').dt.date \n",
    "    tmp2 = pd.to_datetime(X_test['hearing_date'], format='%Y-%m-%d').dt.date\n",
    "    tmp3 = tmp2 - tmp1\n",
    "    X_test['duration'] = tmp3.dt.days \n",
    "    X_test['duration'] = X_test['duration'].where(X_test['duration']>0, other=0)\n",
    "    X_test = X_test.drop(['ticket_issued_date', 'hearing_date'], axis=1) \n",
    "\n",
    "    # create target variable\n",
    "    y_train = X_train['compliance']\n",
    "    X_train = X_train.drop(['compliance'], axis=1)\n",
    "\n",
    "    # Create categorical variables\n",
    "    X_train = X_train.rename(columns={'agency_name': 'agency', 'judgment_amount': 'judgment' })\n",
    "    X_test = X_test.rename(columns={'agency_name': 'agency', 'judgment_amount': 'judgment' })\n",
    "\n",
    "    cat_columns = ['agency', 'disposition'] \n",
    "    df_processed = pd.get_dummies(X_train, prefix_sep=\"__\",\n",
    "                                  columns=cat_columns)\n",
    "\n",
    "    cat_dummies = [col for col in df_processed \n",
    "                   if \"__\" in col \n",
    "                   and col.split(\"__\")[0] in cat_columns]\n",
    "\n",
    "    processed_columns = list(df_processed.columns[:])\n",
    "    df_test_processed = pd.get_dummies(X_test, prefix_sep=\"__\", \n",
    "                                       columns=cat_columns)\n",
    "\n",
    "    # Remove additional columns\n",
    "    for col in df_test_processed.columns:\n",
    "        if (\"__\" in col) and (col.split(\"__\")[0] in cat_columns) and col not in cat_dummies:\n",
    "            df_test_processed.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    for col in cat_dummies:\n",
    "        if col not in df_test_processed.columns:\n",
    "            df_test_processed[col] = 0\n",
    "\n",
    "    df_test_processed = df_test_processed[processed_columns]\n",
    "\n",
    "    \n",
    "    # Validation step to arrive at the right parameters left here to document model choice\n",
    "    params = {'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2] , 'max_depth': [2,3,4,5,6]}\n",
    "    gsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(random_state = 0), \n",
    "                            param_grid = params, scoring='roc_auc', cv=5)\n",
    "\n",
    "    #Fit the algorithm on the data\n",
    "    gsearch3.fit(df_processed, y_train)\n",
    "\n",
    "    print(\"The best parameters for this model will be: learning rate\",  gsearch3.best_params_['learning_rate'],\n",
    "          \"depth\", gsearch3.best_params_['max_depth'])\n",
    "    print(\"The AUC for this model was {:.2f}\".format(gsearch3.best_score_))\n",
    "\n",
    "    # Rather than a train test split, we'll to a 5-fold cross validation to evaluate the model\n",
    "    # The objective set by the assignment was to beat 0.75\n",
    "    cv_score = cross_validate(gsearch3.best_estimator_, df_processed,\n",
    "                              y_train, cv=5, scoring='roc_auc')\n",
    "    print(\"AUC Cross validated best {:.2f} and worse {:.2f} scores\".format(max(cv_score['test_score']), min(cv_score['test_score'])))\n",
    "    \n",
    "    y_hat = gsearch3.predict_proba(df_test_processed)[:,1]\n",
    "    bm = pd.Series(data=y_hat, index=test_ticket_id)\n",
    "    return \"The variable bm was returned for automated grading\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters for this model will be: learning rate 0.15 depth 4\n",
      "The AUC for this model was 0.79\n",
      "AUC Cross validated best 0.84 and worse 0.76 scores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The variable bm was returned for automated grading'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_predictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grading output was:  \n",
    "`Your AUC of 0.800840003561 was awarded`"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "nNS8l",
   "launcher_item_id": "yWWk7",
   "part_id": "w8BSS"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
